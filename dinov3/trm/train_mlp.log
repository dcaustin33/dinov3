/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Currently logged in as: dcaustin33 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in checkpoints/mlp_imagenet/wandb/run-20251015_103736-yfbo4lec
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mlp_imagenet
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dcaustin33/trm-imagenet
wandb: üöÄ View run at https://wandb.ai/dcaustin33/trm-imagenet/runs/yfbo4lec
/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 14 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/home/derek_austin/dinov3/dinov3/trm/train.py:526: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if args.amp and args.device == 'cuda' else None

================================================================================
Training Configuration
================================================================================
  activation: swiglu
  amp: True
  backbone: vit_small
  backbone_checkpoint: /home/derek_austin/dinov3/pretrained_checkpoints/dinov3_vits16_pretrain.pth
  batch_size: 256
  data_root: /home/derek_austin/dinov3/imagenet
  dataset: imagenet
  device: cuda
  epochs: 10
  eval_batch_size: 256
  eval_freq: 1
  experiment_name: mlp_imagenet
  freeze_backbone: True
  grad_clip: 1.0
  hidden_dim_multiplier: 2.0
  images_dtype: torch.bfloat16
  latent_y_dim: 512
  latent_z_dim: 512
  learning_rate: 0.0001
  log_interval: 50
  mlp_num_layers: 1
  momentum: 0.9
  n_latent_reasoning_steps: 3
  n_supervision: 4
  num_classes: 1000
  num_workers: 14
  optimizer: adamw
  patch_size: 14
  pin_memory: True
  resolution: 512
  resume: None
  save_dir: ./checkpoints
  save_freq: 2
  save_path: checkpoints/mlp_imagenet
  seed: 42
  t_recursion_steps: 2
  use_simple_mlp: True
  use_wandb: True
  wandb_entity: None
  wandb_project: trm-imagenet
  warmup_epochs: 1
  weight_decay: 0.0
================================================================================

Saved configuration to checkpoints/mlp_imagenet/config.json

Initialized wandb: mlp_imagenet
  Project: trm-imagenet
  URL: https://wandb.ai/dcaustin33/trm-imagenet/runs/yfbo4lec

Creating data loaders...
[Warning] Custom resolution 512 used instead of default 448 (for 1024 tokens).
Loading ImageNet dataset from /home/derek_austin/dinov3/imagenet
Train dataset: 1281167 images
Val dataset: 50000 images
Train batches: 5004
Val batches: 196

Building model...
Loading backbone checkpoint from /home/derek_austin/dinov3/pretrained_checkpoints/dinov3_vits16_pretrain.pth
Freezing backbone parameters
Using simple MLP classifier with 1 hidden layers
Total parameters: 25,025,128
Trainable parameters: 3,423,976

Building optimizer...
Optimizing only TRM parameters (backbone frozen)
mlp.0.weight torch.Size([768, 384])
mlp.0.bias torch.Size([768])
mlp.1.gate_up_proj.weight torch.Size([2048, 768])
mlp.1.down_proj.weight torch.Size([768, 1024])
mlp.2.weight torch.Size([1000, 768])
mlp.2.bias torch.Size([1000])
Gradient clipping enabled with max norm: 1.0
Using warmup for 1 epochs + cosine decay

================================================================================
Starting Training
================================================================================


Epoch 1/10
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/derek_austin/dinov3/dinov3/trm/train.py", line 584, in <module>
    main()
  File "/home/derek_austin/dinov3/dinov3/trm/train.py", line 544, in main
    train_metrics = train_epoch(model, train_loader, optimizer, scheduler, epoch, args, scaler)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/derek_austin/dinov3/dinov3/trm/train.py", line 222, in train_epoch
    for batch_idx, (images, labels) in enumerate(train_loader):
                                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1489, in _next_data
    return self._process_data(data, worker_id)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1551, in _process_data
    data.reraise()
  File "/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/_utils.py", line 769, in reraise
    raise exception
torch.AcceleratorError: Caught AcceleratorError in pin memory thread for device 0.
Original Traceback (most recent call last):
  File "/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 43, in do_one_step
    data = pin_memory(data, device)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 90, in pin_memory
    pin_memory(sample, device) for sample in data
    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 66, in pin_memory
    return data.pin_memory(device)
           ^^^^^^^^^^^^^^^^^^^^^^^
torch.AcceleratorError: CUDA error: invalid argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mmlp_imagenet[0m at: [34m[0m
Exception in thread Thread-3 (_pin_memory_loop):
socket.send() raised exception.
Exception ignored in atexit callbackException ignored in sys.unraisablehook