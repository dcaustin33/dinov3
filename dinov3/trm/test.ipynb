{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6431ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [-8.8206e-02,  1.3895e-01,  2.8990e-03, -7.4096e-02, -5.9568e-03,\n",
      "          3.8062e-02,  4.7087e-02, -5.5109e-02,  7.3397e-02, -1.1877e-01,\n",
      "          7.1411e-02,  9.0347e-03,  6.8171e-02,  5.2951e-02,  3.3600e-02,\n",
      "         -2.3978e-03, -9.0476e-04, -8.5729e-02,  9.9391e-02, -6.9879e-03,\n",
      "          6.3970e-02,  2.6974e-02, -4.5227e-02, -7.3767e-02, -3.1551e-02,\n",
      "          8.6635e-02,  1.2341e-01,  4.3791e-02,  5.0437e-02, -3.9473e-02],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 1.0838e-01, -1.6801e-01, -9.9196e-03,  9.0657e-02,  1.2972e-02,\n",
      "         -4.6156e-02, -5.3829e-02,  5.6300e-02, -8.9151e-02,  1.4438e-01,\n",
      "         -8.8202e-02, -1.1498e-02, -8.4520e-02, -6.5358e-02, -4.1487e-02,\n",
      "          1.9400e-03,  1.2808e-03,  1.0523e-01, -1.2276e-01,  8.7111e-03,\n",
      "         -8.0420e-02, -3.3645e-02,  5.5838e-02,  9.1652e-02,  3.8465e-02,\n",
      "         -1.0668e-01, -1.5304e-01, -5.4736e-02, -6.3108e-02,  4.8089e-02],\n",
      "        [ 1.7791e-01, -2.9952e-01,  3.9125e-02,  1.5197e-01, -2.7981e-02,\n",
      "         -8.1010e-02, -1.2369e-01,  1.9198e-01, -1.5516e-01,  2.4995e-01,\n",
      "         -1.4061e-01, -1.5448e-02, -1.3186e-01, -1.0485e-01, -6.5776e-02,\n",
      "          1.1900e-02,  3.1722e-04,  1.7374e-01, -1.9570e-01,  1.3205e-02,\n",
      "         -1.1602e-01, -5.1008e-02,  8.9617e-02,  1.4163e-01,  6.5786e-02,\n",
      "         -1.7368e-01, -2.3837e-01, -8.1254e-02, -9.4169e-02,  8.2394e-02],\n",
      "        [ 1.4475e-01, -2.3920e-01,  2.1188e-02,  1.2280e-01, -1.3305e-02,\n",
      "         -6.4812e-02, -9.4104e-02,  1.3717e-01, -1.2435e-01,  2.0015e-01,\n",
      "         -1.1500e-01, -1.3268e-02, -1.0828e-01, -8.5936e-02, -5.3334e-02,\n",
      "          7.9475e-03,  2.5074e-04,  1.4129e-01, -1.6006e-01,  1.0948e-02,\n",
      "         -9.7318e-02, -4.2476e-02,  7.3624e-02,  1.1694e-01,  5.3023e-02,\n",
      "         -1.4226e-01, -1.9571e-01, -6.7202e-02, -7.8966e-02,  6.6223e-02],\n",
      "        [-2.3546e-01,  3.8548e-01, -2.6247e-02, -1.9966e-01,  1.4325e-02,\n",
      "          1.0479e-01,  1.4743e-01, -2.0821e-01,  2.0128e-01, -3.2480e-01,\n",
      "          1.8801e-01,  2.2026e-02,  1.7767e-01,  1.3990e-01,  8.8092e-02,\n",
      "         -1.1730e-02, -1.2233e-03, -2.2949e-01,  2.6167e-01, -1.7981e-02,\n",
      "          1.6091e-01,  6.9462e-02, -1.1957e-01, -1.9151e-01, -8.5849e-02,\n",
      "          2.3056e-01,  3.2136e-01,  1.1147e-01,  1.2900e-01, -1.0745e-01],\n",
      "        [-2.2749e-01,  3.7291e-01, -2.6487e-02, -1.9295e-01,  1.4844e-02,\n",
      "          1.0135e-01,  1.4318e-01, -2.0319e-01,  1.9463e-01, -3.1401e-01,\n",
      "          1.8154e-01,  2.1214e-02,  1.7149e-01,  1.3512e-01,  8.5018e-02,\n",
      "         -1.1506e-02, -1.1214e-03, -2.2175e-01,  2.5267e-01, -1.7349e-02,\n",
      "          1.5513e-01,  6.7035e-02, -1.1550e-01, -1.8485e-01, -8.2996e-02,\n",
      "          2.2277e-01,  3.1017e-01,  1.0748e-01,  1.2448e-01, -1.0387e-01]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    z_latent_dim = z_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:x_dim+y_latent_dim+z_latent_dim]\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # Don't modify y_latent and z_latent in place within no_grad\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        with torch.no_grad():\n",
    "            y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach(), net=net)\n",
    "        y_latent = y_latent_new\n",
    "        z_latent = z_latent_new\n",
    "    y_latent = y_latent.requires_grad_(True)\n",
    "    z_latent = z_latent.requires_grad_(True)\n",
    "    y_latent, z_latent = latent_recursion(x, y_latent, z_latent)\n",
    "    return y_latent, z_latent\n",
    "\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent = torch.randn(1,10).cuda()\n",
    "z_latent = torch.randn(1,10).cuda()\n",
    "# with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "y_latent, z_latent = deep_recursion(x, y_latent, z_latent, net=net)\n",
    "example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "loss = F.cross_entropy(y_latent, example_class)\n",
    "loss.backward()\n",
    "print(net[0].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bb45de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [-0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., -0., 0., 0., 0.,\n",
      "         0., -0., 0., -0., -0., 0.],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [-0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., -0., 0., 0., 0.,\n",
      "         0., -0., 0., -0., -0., 0.],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [-0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., -0., 0., 0., 0.,\n",
      "         0., -0., 0., -0., -0., 0.],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Network and functions are unchanged ---\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor,\n",
    "    y_latent: torch.Tensor,\n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:] # Simplified slicing\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor,\n",
    "    y_latent: torch.Tensor,\n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # This loop runs in default precision (float32) and without gradients\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        with torch.no_grad():\n",
    "            y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach())\n",
    "        y_latent = y_latent_new\n",
    "        z_latent = z_latent_new\n",
    "\n",
    "    # Make the float32 tensors require gradients\n",
    "    y_latent = y_latent.requires_grad_(True)\n",
    "    z_latent = z_latent.requires_grad_(True)\n",
    "\n",
    "    # **THE FIX: Apply autocast ONLY to the final differentiable step**\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        y_latent_final, z_latent_final = latent_recursion(x, y_latent, z_latent)\n",
    "\n",
    "    return y_latent_final, z_latent_final\n",
    "\n",
    "# --- Training loop ---\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent_start = torch.randn(1,10).cuda()\n",
    "z_latent_start = torch.randn(1,10).cuda()\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# No top-level autocast here\n",
    "y_latent_out, z_latent_out = deep_recursion(x, y_latent_start, z_latent_start)\n",
    "example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "# The loss calculation can also be inside the autocast block if preferred,\n",
    "# but it works here too because y_latent_out is now correctly connected.\n",
    "# For consistency, we put the autocast around the operation that needs it.\n",
    "loss = F.cross_entropy(y_latent_out, example_class)\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "print(net[0].weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
