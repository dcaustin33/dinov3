{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6431ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [-8.8206e-02,  1.3895e-01,  2.8990e-03, -7.4096e-02, -5.9568e-03,\n",
      "          3.8062e-02,  4.7087e-02, -5.5109e-02,  7.3397e-02, -1.1877e-01,\n",
      "          7.1411e-02,  9.0347e-03,  6.8171e-02,  5.2951e-02,  3.3600e-02,\n",
      "         -2.3978e-03, -9.0476e-04, -8.5729e-02,  9.9391e-02, -6.9879e-03,\n",
      "          6.3970e-02,  2.6974e-02, -4.5227e-02, -7.3767e-02, -3.1551e-02,\n",
      "          8.6635e-02,  1.2341e-01,  4.3791e-02,  5.0437e-02, -3.9473e-02],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 1.0838e-01, -1.6801e-01, -9.9196e-03,  9.0657e-02,  1.2972e-02,\n",
      "         -4.6156e-02, -5.3829e-02,  5.6300e-02, -8.9151e-02,  1.4438e-01,\n",
      "         -8.8202e-02, -1.1498e-02, -8.4520e-02, -6.5358e-02, -4.1487e-02,\n",
      "          1.9400e-03,  1.2808e-03,  1.0523e-01, -1.2276e-01,  8.7111e-03,\n",
      "         -8.0420e-02, -3.3645e-02,  5.5838e-02,  9.1652e-02,  3.8465e-02,\n",
      "         -1.0668e-01, -1.5304e-01, -5.4736e-02, -6.3108e-02,  4.8089e-02],\n",
      "        [ 1.7791e-01, -2.9952e-01,  3.9125e-02,  1.5197e-01, -2.7981e-02,\n",
      "         -8.1010e-02, -1.2369e-01,  1.9198e-01, -1.5516e-01,  2.4995e-01,\n",
      "         -1.4061e-01, -1.5448e-02, -1.3186e-01, -1.0485e-01, -6.5776e-02,\n",
      "          1.1900e-02,  3.1722e-04,  1.7374e-01, -1.9570e-01,  1.3205e-02,\n",
      "         -1.1602e-01, -5.1008e-02,  8.9617e-02,  1.4163e-01,  6.5786e-02,\n",
      "         -1.7368e-01, -2.3837e-01, -8.1254e-02, -9.4169e-02,  8.2394e-02],\n",
      "        [ 1.4475e-01, -2.3920e-01,  2.1188e-02,  1.2280e-01, -1.3305e-02,\n",
      "         -6.4812e-02, -9.4104e-02,  1.3717e-01, -1.2435e-01,  2.0015e-01,\n",
      "         -1.1500e-01, -1.3268e-02, -1.0828e-01, -8.5936e-02, -5.3334e-02,\n",
      "          7.9475e-03,  2.5074e-04,  1.4129e-01, -1.6006e-01,  1.0948e-02,\n",
      "         -9.7318e-02, -4.2476e-02,  7.3624e-02,  1.1694e-01,  5.3023e-02,\n",
      "         -1.4226e-01, -1.9571e-01, -6.7202e-02, -7.8966e-02,  6.6223e-02],\n",
      "        [-2.3546e-01,  3.8548e-01, -2.6247e-02, -1.9966e-01,  1.4325e-02,\n",
      "          1.0479e-01,  1.4743e-01, -2.0821e-01,  2.0128e-01, -3.2480e-01,\n",
      "          1.8801e-01,  2.2026e-02,  1.7767e-01,  1.3990e-01,  8.8092e-02,\n",
      "         -1.1730e-02, -1.2233e-03, -2.2949e-01,  2.6167e-01, -1.7981e-02,\n",
      "          1.6091e-01,  6.9462e-02, -1.1957e-01, -1.9151e-01, -8.5849e-02,\n",
      "          2.3056e-01,  3.2136e-01,  1.1147e-01,  1.2900e-01, -1.0745e-01],\n",
      "        [-2.2749e-01,  3.7291e-01, -2.6487e-02, -1.9295e-01,  1.4844e-02,\n",
      "          1.0135e-01,  1.4318e-01, -2.0319e-01,  1.9463e-01, -3.1401e-01,\n",
      "          1.8154e-01,  2.1214e-02,  1.7149e-01,  1.3512e-01,  8.5018e-02,\n",
      "         -1.1506e-02, -1.1214e-03, -2.2175e-01,  2.5267e-01, -1.7349e-02,\n",
      "          1.5513e-01,  6.7035e-02, -1.1550e-01, -1.8485e-01, -8.2996e-02,\n",
      "          2.2277e-01,  3.1017e-01,  1.0748e-01,  1.2448e-01, -1.0387e-01]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    z_latent_dim = z_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:x_dim+y_latent_dim+z_latent_dim]\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # Don't modify y_latent and z_latent in place within no_grad\n",
    "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        for _ in range(t_recursion_steps - 1):\n",
    "            with torch.no_grad():\n",
    "                y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach(), net=net)\n",
    "            y_latent = y_latent_new\n",
    "            z_latent = z_latent_new\n",
    "    y_latent = y_latent.requires_grad_(True)\n",
    "    z_latent = z_latent.requires_grad_(True)\n",
    "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        y_latent, z_latent = latent_recursion(x, y_latent, z_latent, net=net)\n",
    "    return y_latent, z_latent\n",
    "\n",
    "def deep_recursion(x, y_latent, z_latent, t_recursion_steps=2, net=net):\n",
    "    # burn-in steps: no graph, just values\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        with torch.no_grad():\n",
    "            y_latent, z_latent = latent_recursion(x, y_latent, z_latent, net=net)\n",
    "\n",
    "    # final step: build graph through net\n",
    "    # (no need to flip requires_grad_ on latents unless you want their .grad)\n",
    "    y_latent = y_latent.requires_grad_(True)\n",
    "    z_latent = z_latent.requires_grad_(True)\n",
    "    with torch.set_grad_enabled(True):\n",
    "        y_latent, z_latent = latent_recursion(x, y_latent, z_latent, net=net)\n",
    "    return y_latent, z_latent\n",
    "\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent = torch.randn(1,10).cuda()\n",
    "z_latent = torch.randn(1,10).cuda()\n",
    "y_latent, z_latent = deep_recursion(x, y_latent, z_latent, net=net)\n",
    "example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "loss = F.cross_entropy(y_latent, example_class)\n",
    "loss.backward()\n",
    "print(net[0].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bb45de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [-8.8379e-02,  1.3867e-01,  3.2959e-03, -7.3730e-02, -6.1646e-03,\n",
      "          3.8086e-02,  4.6875e-02, -5.4443e-02,  7.2754e-02, -1.1914e-01,\n",
      "          7.1289e-02,  9.0332e-03,  6.7383e-02,  5.2979e-02,  3.3691e-02,\n",
      "         -2.3651e-03, -9.3079e-04, -8.5938e-02,  9.9609e-02, -6.9580e-03,\n",
      "          6.3477e-02,  2.6978e-02, -4.5410e-02, -7.3730e-02, -3.1494e-02,\n",
      "          8.6426e-02,  1.2402e-01,  4.3701e-02,  5.0293e-02, -3.9307e-02],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 1.0986e-01, -1.6797e-01, -9.6436e-03,  9.0820e-02,  1.2634e-02,\n",
      "         -4.6387e-02, -5.5176e-02,  5.7373e-02, -8.9355e-02,  1.4551e-01,\n",
      "         -8.8867e-02, -1.1536e-02, -8.4473e-02, -6.5430e-02, -4.1992e-02,\n",
      "          2.0447e-03,  1.2970e-03,  1.0596e-01, -1.2354e-01,  8.6670e-03,\n",
      "         -8.0566e-02, -3.3691e-02,  5.6396e-02,  9.2285e-02,  3.9062e-02,\n",
      "         -1.0693e-01, -1.5332e-01, -5.5176e-02, -6.2988e-02,  4.8584e-02],\n",
      "        [ 1.7969e-01, -3.0273e-01,  3.9307e-02,  1.5332e-01, -2.8320e-02,\n",
      "         -8.1543e-02, -1.2500e-01,  1.9336e-01, -1.5625e-01,  2.5195e-01,\n",
      "         -1.4258e-01, -1.5625e-02, -1.3281e-01, -1.0596e-01, -6.6406e-02,\n",
      "          1.2024e-02,  3.0327e-04,  1.7578e-01, -1.9727e-01,  1.3245e-02,\n",
      "         -1.1670e-01, -5.1514e-02,  9.0820e-02,  1.4258e-01,  6.6406e-02,\n",
      "         -1.7480e-01, -2.4023e-01, -8.2031e-02, -9.4727e-02,  8.3008e-02],\n",
      "        [ 1.4551e-01, -2.3926e-01,  2.0996e-02,  1.2256e-01, -1.3367e-02,\n",
      "         -6.4941e-02, -9.4727e-02,  1.3672e-01, -1.2402e-01,  2.0020e-01,\n",
      "         -1.1523e-01, -1.3306e-02, -1.0791e-01, -8.6426e-02, -5.3711e-02,\n",
      "          7.9346e-03,  2.5368e-04,  1.4160e-01, -1.6016e-01,  1.0925e-02,\n",
      "         -9.7168e-02, -4.2480e-02,  7.3730e-02,  1.1670e-01,  5.3467e-02,\n",
      "         -1.4258e-01, -1.9531e-01, -6.7383e-02, -7.8613e-02,  6.6406e-02],\n",
      "        [-2.3633e-01,  3.8477e-01, -2.6367e-02, -1.9922e-01,  1.4404e-02,\n",
      "          1.0498e-01,  1.4746e-01, -2.0801e-01,  2.0117e-01, -3.2617e-01,\n",
      "          1.8848e-01,  2.1973e-02,  1.7676e-01,  1.3867e-01,  8.8379e-02,\n",
      "         -1.1841e-02, -1.2283e-03, -2.3047e-01,  2.6172e-01, -1.7700e-02,\n",
      "          1.6016e-01,  6.9336e-02, -1.1914e-01, -1.9141e-01, -8.6426e-02,\n",
      "          2.2949e-01,  3.2227e-01,  1.1182e-01,  1.2891e-01, -1.0742e-01],\n",
      "        [-2.2754e-01,  3.7109e-01, -2.6001e-02, -1.9141e-01,  1.4832e-02,\n",
      "          1.0107e-01,  1.4355e-01, -2.0215e-01,  1.9434e-01, -3.1445e-01,\n",
      "          1.8164e-01,  2.1240e-02,  1.7090e-01,  1.3477e-01,  8.4961e-02,\n",
      "         -1.1475e-02, -1.1292e-03, -2.2168e-01,  2.5195e-01, -1.7090e-02,\n",
      "          1.5430e-01,  6.6895e-02, -1.1572e-01, -1.8457e-01, -8.3008e-02,\n",
      "          2.2168e-01,  3.1055e-01,  1.0742e-01,  1.2354e-01, -1.0352e-01]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    z_latent_dim = z_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:x_dim+y_latent_dim+z_latent_dim]\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # Don't modify y_latent and z_latent in place within no_grad\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        net.requires_grad_(False)\n",
    "        y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach(), net=net)\n",
    "        net.requires_grad_(True)\n",
    "        y_latent = y_latent_new\n",
    "        z_latent = z_latent_new\n",
    "    y_latent, z_latent = latent_recursion(x, y_latent, z_latent)\n",
    "    return y_latent, z_latent\n",
    "\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent = torch.randn(1,10).cuda()\n",
    "z_latent = torch.randn(1,10).cuda()\n",
    "with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    y_latent, z_latent = deep_recursion(x, y_latent, z_latent, net=net)\n",
    "    example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "    loss = F.cross_entropy(y_latent, example_class)\n",
    "    loss.backward()\n",
    "    print(net[0].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bee98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside final pass - grad enabled? True\n",
      "inside final pass - y_out.requires_grad? False\n",
      "after final call - y_out.requires_grad? False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m target = torch.randint(\u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m, (\u001b[32m1\u001b[39m,), device=device)\n\u001b[32m     43\u001b[39m loss = F.cross_entropy(y_out, target)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(net[\u001b[32m0\u001b[39m].weight.grad)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnet[0].weight.grad is None?\u001b[39m\u001b[33m\"\u001b[39m, net[\u001b[32m0\u001b[39m].weight.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dinov3/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dinov3/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dinov3/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\"\n",
    "\n",
    "net = nn.Sequential(nn.Linear(30, 10), nn.ReLU(), nn.Linear(10, 30)).to(device)\n",
    "\n",
    "def latent_recursion(x, y_latent, z_latent, n=3, net=net):\n",
    "    x_dim, y_dim, z_dim = x.shape[-1], y_latent.shape[-1], z_latent.shape[-1]\n",
    "    inp = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n):\n",
    "        out = net(inp)\n",
    "        inp = out + inp\n",
    "    y = out[:, x_dim:x_dim+y_dim]\n",
    "    z = out[:, x_dim+y_dim:x_dim+y_dim+z_dim]\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(x, y_latent, z_latent, t=2, net=net):\n",
    "    # warm-up: explicitly no grad\n",
    "    for _ in range(t - 1):\n",
    "        with torch.no_grad():\n",
    "            y_latent, z_latent = latent_recursion(x, y_latent, z_latent, net=net)\n",
    "\n",
    "    # FINAL pass: explicitly enable grad, and DEBUG inside\n",
    "    print(\"inside final pass - grad enabled?\", torch.is_grad_enabled())\n",
    "    y_out, z_out = latent_recursion(x, y_latent, z_latent, net=net)\n",
    "    print(\"inside final pass - y_out.requires_grad?\", y_out.requires_grad)\n",
    "    return y_out, z_out\n",
    "\n",
    "x = torch.randn(1,10, device=device)\n",
    "y0 = torch.randn(1,10, device=device)\n",
    "z0 = torch.randn(1,10, device=device)\n",
    "\n",
    "# Try WITHOUT autocast first\n",
    "y_out, z_out = deep_recursion(x, y0, z0, net=net)\n",
    "print(\"after final call - y_out.requires_grad?\", y_out.requires_grad)\n",
    "\n",
    "target = torch.randint(0, 10, (1,), device=device)\n",
    "loss = F.cross_entropy(y_out, target)\n",
    "loss.backward()\n",
    "print(net[0].weight.grad)\n",
    "print(\"net[0].weight.grad is None?\", net[0].weight.grad is None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
