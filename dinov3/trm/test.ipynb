{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6431ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.9366e-01, -4.4403e-01,  2.9051e-01,  4.8551e-01, -1.5548e-01,\n",
      "         -1.0718e-01, -2.3545e-01,  1.6691e-01,  3.5807e-01,  4.8783e-02,\n",
      "         -1.7829e-02,  2.6798e-01,  1.1731e-01,  1.4514e-01, -1.2646e-01,\n",
      "         -1.9012e-02,  1.9878e-01, -1.5690e-02, -3.9625e-02, -1.4597e-01,\n",
      "          1.9006e-01, -1.1753e-01,  3.0696e-02, -2.9123e-02,  2.5830e-01,\n",
      "          6.5135e-03, -7.6369e-04,  2.8767e-02,  4.2457e-02,  1.0483e-01],\n",
      "        [ 4.0713e-01,  3.8717e-01, -2.0758e-01, -4.3835e-01,  1.1208e-01,\n",
      "          6.9512e-02,  2.3013e-01, -1.5924e-01, -2.3239e-01, -8.6032e-02,\n",
      "          3.8398e-02, -3.2245e-01, -1.3493e-01, -1.8974e-01,  1.6468e-01,\n",
      "          2.5330e-02, -2.5046e-01,  1.1611e-02,  2.3038e-02,  1.9859e-01,\n",
      "         -2.0965e-01,  1.6881e-01, -4.0080e-02,  2.4384e-02, -3.0998e-01,\n",
      "         -2.1479e-02, -1.1683e-02, -2.7006e-02, -4.4746e-02, -1.2311e-01],\n",
      "        [ 4.0414e-01,  3.5511e-01, -1.6861e-01, -4.0893e-01,  9.1237e-02,\n",
      "          5.2068e-02,  2.2278e-01, -1.5246e-01, -1.7402e-01, -9.9917e-02,\n",
      "          4.5876e-02, -3.3890e-01, -1.3992e-01, -2.0450e-01,  1.7809e-01,\n",
      "          2.7482e-02, -2.6797e-01,  1.0251e-02,  1.5608e-02,  2.1718e-01,\n",
      "         -2.1352e-01,  1.8661e-01, -4.4120e-02,  2.1736e-02, -3.2502e-01,\n",
      "         -2.7530e-02, -1.7283e-02, -2.5089e-02, -4.5117e-02, -1.2809e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [ 2.6941e-01,  3.3471e-01, -2.3892e-01, -3.5900e-01,  1.2681e-01,\n",
      "          9.0748e-02,  1.6647e-01, -1.1963e-01, -3.0285e-01, -1.8318e-02,\n",
      "          3.0992e-03, -1.6477e-01, -7.5244e-02, -8.2511e-02,  7.3522e-02,\n",
      "          1.0656e-02, -1.1879e-01,  1.3722e-02,  3.4819e-02,  8.0774e-02,\n",
      "         -1.2465e-01,  6.0006e-02, -1.9056e-02,  2.2141e-02, -1.5834e-01,\n",
      "          1.4582e-03,  4.7970e-03, -1.9837e-02, -2.9308e-02, -6.5328e-02],\n",
      "        [-2.3914e-01, -1.4904e-01,  2.0688e-02,  1.8843e-01, -1.3089e-02,\n",
      "          3.9658e-03, -1.2085e-01,  7.9166e-02, -1.2836e-02,  8.9143e-02,\n",
      "         -4.4589e-02,  2.3825e-01,  9.3602e-02,  1.5448e-01, -1.3275e-01,\n",
      "         -2.1002e-02,  1.9481e-01, -1.2164e-03,  5.9982e-03, -1.6773e-01,\n",
      "          1.3727e-01, -1.5068e-01,  3.1675e-02, -8.2249e-03,  2.2874e-01,\n",
      "          2.8425e-02,  1.9816e-02,  1.3468e-02,  2.6841e-02,  8.8367e-02],\n",
      "        [-4.1039e-02, -5.8895e-02,  4.7052e-02,  6.0992e-02, -2.4119e-02,\n",
      "         -1.8001e-02, -2.6332e-02,  1.9290e-02,  5.9753e-02, -9.2380e-04,\n",
      "          2.4245e-03,  2.1063e-02,  1.0950e-02,  8.3934e-03, -9.3266e-03,\n",
      "         -1.0868e-03,  1.5602e-02, -3.8599e-03, -7.3043e-03, -8.6392e-03,\n",
      "          1.7773e-02, -3.6862e-03,  4.0077e-03, -3.6981e-03,  1.9284e-02,\n",
      "         -1.3511e-03, -7.7155e-04,  2.0415e-03,  5.1350e-03,  8.1760e-03],\n",
      "        [-1.5197e-01, -1.9285e-01,  1.4022e-01,  2.0573e-01, -7.3987e-02,\n",
      "         -5.3326e-02, -9.4398e-02,  6.8028e-02,  1.7780e-01,  8.4327e-03,\n",
      "         -2.6635e-04,  9.0877e-02,  4.2182e-02,  4.4406e-02, -4.0514e-02,\n",
      "         -5.7363e-03,  6.5730e-02, -8.6457e-03, -2.0664e-02, -4.3688e-02,\n",
      "          6.9692e-02, -3.1058e-02,  1.1315e-02, -1.2656e-02,  8.6842e-02,\n",
      "         -1.4001e-03, -2.7267e-03,  1.0688e-02,  1.6875e-02,  3.5942e-02],\n",
      "        [-5.8640e-01, -5.6998e-01,  3.1504e-01,  6.4202e-01, -1.6957e-01,\n",
      "         -1.0707e-01, -3.3357e-01,  2.3151e-01,  3.5778e-01,  1.1789e-01,\n",
      "         -5.1635e-02,  4.5702e-01,  1.9236e-01,  2.6656e-01, -2.3206e-01,\n",
      "         -3.5543e-02,  3.5394e-01, -1.7967e-02, -3.6265e-02, -2.7844e-01,\n",
      "          2.9980e-01, -2.3496e-01,  5.7027e-02, -3.5994e-02,  4.3909e-01,\n",
      "          2.8588e-02,  1.5170e-02,  3.8907e-02,  6.4579e-02,  1.7474e-01]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    z_latent_dim = z_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:x_dim+y_latent_dim+z_latent_dim]\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # Don't modify y_latent and z_latent in place within no_grad\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        with torch.no_grad():\n",
    "            y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach())\n",
    "        y_latent = y_latent_new\n",
    "        z_latent = z_latent_new\n",
    "    y_latent = y_latent.requires_grad_(True)\n",
    "    z_latent = z_latent.requires_grad_(True)\n",
    "    y_latent, z_latent = latent_recursion(x, y_latent, z_latent)\n",
    "    return y_latent, z_latent\n",
    "\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent = torch.randn(1,10).cuda()\n",
    "z_latent = torch.randn(1,10).cuda()\n",
    "scaler = torch.amp.GradScaler()\n",
    "# with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "y_latent, z_latent = deep_recursion(x, y_latent, z_latent)\n",
    "example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "loss = F.cross_entropy(y_latent, example_class)\n",
    "# scaler.scale(loss).backward()\n",
    "loss.backward()\n",
    "print(net[0].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bb45de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [-0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., -0., 0., 0., 0.,\n",
      "         0., -0., 0., -0., -0., 0.],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [-0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., -0., 0., 0., 0.,\n",
      "         0., -0., 0., -0., -0., 0.],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [-0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., -0., 0., 0., 0.,\n",
      "         0., -0., 0., -0., -0., 0.],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Network and functions are unchanged ---\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor,\n",
    "    y_latent: torch.Tensor,\n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:] # Simplified slicing\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor,\n",
    "    y_latent: torch.Tensor,\n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # This loop runs in default precision (float32) and without gradients\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        with torch.no_grad():\n",
    "            y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach())\n",
    "        y_latent = y_latent_new\n",
    "        z_latent = z_latent_new\n",
    "\n",
    "    # Make the float32 tensors require gradients\n",
    "    y_latent = y_latent.requires_grad_(True)\n",
    "    z_latent = z_latent.requires_grad_(True)\n",
    "\n",
    "    # **THE FIX: Apply autocast ONLY to the final differentiable step**\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        y_latent_final, z_latent_final = latent_recursion(x, y_latent, z_latent)\n",
    "\n",
    "    return y_latent_final, z_latent_final\n",
    "\n",
    "# --- Training loop ---\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent_start = torch.randn(1,10).cuda()\n",
    "z_latent_start = torch.randn(1,10).cuda()\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# No top-level autocast here\n",
    "y_latent_out, z_latent_out = deep_recursion(x, y_latent_start, z_latent_start)\n",
    "example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "# The loss calculation can also be inside the autocast block if preferred,\n",
    "# but it works here too because y_latent_out is now correctly connected.\n",
    "# For consistency, we put the autocast around the operation that needs it.\n",
    "loss = F.cross_entropy(y_latent_out, example_class)\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "print(net[0].weight.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
