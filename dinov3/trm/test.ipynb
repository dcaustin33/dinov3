{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6431ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    z_latent_dim = z_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:x_dim+y_latent_dim+z_latent_dim]\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # Don't modify y_latent and z_latent in place within no_grad\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        with torch.no_grad():\n",
    "            y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach(), net=net)\n",
    "        y_latent = y_latent_new\n",
    "        z_latent = z_latent_new\n",
    "    y_latent = y_latent.requires_grad_(True)\n",
    "    z_latent = z_latent.requires_grad_(True)\n",
    "    y_latent, z_latent = latent_recursion(x, y_latent, z_latent)\n",
    "    return y_latent, z_latent\n",
    "\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent = torch.randn(1,10).cuda()\n",
    "z_latent = torch.randn(1,10).cuda()\n",
    "with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    y_latent, z_latent = deep_recursion(x, y_latent, z_latent, net=net)\n",
    "    example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "    loss = F.cross_entropy(y_latent, example_class)\n",
    "    loss.backward()\n",
    "    print(net[0].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bb45de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [-8.8379e-02,  1.3867e-01,  3.2959e-03, -7.3730e-02, -6.1646e-03,\n",
      "          3.8086e-02,  4.6875e-02, -5.4443e-02,  7.2754e-02, -1.1914e-01,\n",
      "          7.1289e-02,  9.0332e-03,  6.7383e-02,  5.2979e-02,  3.3691e-02,\n",
      "         -2.3651e-03, -9.3079e-04, -8.5938e-02,  9.9609e-02, -6.9580e-03,\n",
      "          6.3477e-02,  2.6978e-02, -4.5410e-02, -7.3730e-02, -3.1494e-02,\n",
      "          8.6426e-02,  1.2402e-01,  4.3701e-02,  5.0293e-02, -3.9307e-02],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
      "        [ 1.0986e-01, -1.6797e-01, -9.6436e-03,  9.0820e-02,  1.2634e-02,\n",
      "         -4.6387e-02, -5.5176e-02,  5.7373e-02, -8.9355e-02,  1.4551e-01,\n",
      "         -8.8867e-02, -1.1536e-02, -8.4473e-02, -6.5430e-02, -4.1992e-02,\n",
      "          2.0447e-03,  1.2970e-03,  1.0596e-01, -1.2354e-01,  8.6670e-03,\n",
      "         -8.0566e-02, -3.3691e-02,  5.6396e-02,  9.2285e-02,  3.9062e-02,\n",
      "         -1.0693e-01, -1.5332e-01, -5.5176e-02, -6.2988e-02,  4.8584e-02],\n",
      "        [ 1.7969e-01, -3.0273e-01,  3.9307e-02,  1.5332e-01, -2.8320e-02,\n",
      "         -8.1543e-02, -1.2500e-01,  1.9336e-01, -1.5625e-01,  2.5195e-01,\n",
      "         -1.4258e-01, -1.5625e-02, -1.3281e-01, -1.0596e-01, -6.6406e-02,\n",
      "          1.2024e-02,  3.0327e-04,  1.7578e-01, -1.9727e-01,  1.3245e-02,\n",
      "         -1.1670e-01, -5.1514e-02,  9.0820e-02,  1.4258e-01,  6.6406e-02,\n",
      "         -1.7480e-01, -2.4023e-01, -8.2031e-02, -9.4727e-02,  8.3008e-02],\n",
      "        [ 1.4551e-01, -2.3926e-01,  2.0996e-02,  1.2256e-01, -1.3367e-02,\n",
      "         -6.4941e-02, -9.4727e-02,  1.3672e-01, -1.2402e-01,  2.0020e-01,\n",
      "         -1.1523e-01, -1.3306e-02, -1.0791e-01, -8.6426e-02, -5.3711e-02,\n",
      "          7.9346e-03,  2.5368e-04,  1.4160e-01, -1.6016e-01,  1.0925e-02,\n",
      "         -9.7168e-02, -4.2480e-02,  7.3730e-02,  1.1670e-01,  5.3467e-02,\n",
      "         -1.4258e-01, -1.9531e-01, -6.7383e-02, -7.8613e-02,  6.6406e-02],\n",
      "        [-2.3633e-01,  3.8477e-01, -2.6367e-02, -1.9922e-01,  1.4404e-02,\n",
      "          1.0498e-01,  1.4746e-01, -2.0801e-01,  2.0117e-01, -3.2617e-01,\n",
      "          1.8848e-01,  2.1973e-02,  1.7676e-01,  1.3867e-01,  8.8379e-02,\n",
      "         -1.1841e-02, -1.2283e-03, -2.3047e-01,  2.6172e-01, -1.7700e-02,\n",
      "          1.6016e-01,  6.9336e-02, -1.1914e-01, -1.9141e-01, -8.6426e-02,\n",
      "          2.2949e-01,  3.2227e-01,  1.1182e-01,  1.2891e-01, -1.0742e-01],\n",
      "        [-2.2754e-01,  3.7109e-01, -2.6001e-02, -1.9141e-01,  1.4832e-02,\n",
      "          1.0107e-01,  1.4355e-01, -2.0215e-01,  1.9434e-01, -3.1445e-01,\n",
      "          1.8164e-01,  2.1240e-02,  1.7090e-01,  1.3477e-01,  8.4961e-02,\n",
      "         -1.1475e-02, -1.1292e-03, -2.2168e-01,  2.5195e-01, -1.7090e-02,\n",
      "          1.5430e-01,  6.6895e-02, -1.1572e-01, -1.8457e-01, -8.3008e-02,\n",
      "          2.2168e-01,  3.1055e-01,  1.0742e-01,  1.2354e-01, -1.0352e-01]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(30, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 30),\n",
    ").cuda()\n",
    "\n",
    "def latent_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    n_latent_reasoning_steps: int = 3,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    x_dim = x.shape[-1]\n",
    "    y_latent_dim = y_latent.shape[-1]\n",
    "    z_latent_dim = z_latent.shape[-1]\n",
    "    input_tensor = torch.cat([x, y_latent, z_latent], dim=-1)\n",
    "    for _ in range(n_latent_reasoning_steps):\n",
    "        output_tensor = net(input_tensor)\n",
    "        input_tensor = output_tensor + input_tensor\n",
    "    y = output_tensor[:, x_dim:x_dim+y_latent_dim]\n",
    "    z = output_tensor[:, x_dim+y_latent_dim:x_dim+y_latent_dim+z_latent_dim]\n",
    "    return y, z\n",
    "\n",
    "def deep_recursion(\n",
    "    x: torch.Tensor, \n",
    "    y_latent: torch.Tensor, \n",
    "    z_latent: torch.Tensor,\n",
    "    t_recursion_steps: int = 2,\n",
    "    net: nn.Module = net\n",
    "):\n",
    "    # Don't modify y_latent and z_latent in place within no_grad\n",
    "    for _ in range(t_recursion_steps - 1):\n",
    "        net.requires_grad_(False)\n",
    "        y_latent_new, z_latent_new = latent_recursion(x, y_latent.detach(), z_latent.detach(), net=net)\n",
    "        net.requires_grad_(True)\n",
    "        y_latent = y_latent_new\n",
    "        z_latent = z_latent_new\n",
    "    y_latent, z_latent = latent_recursion(x, y_latent, z_latent)\n",
    "    return y_latent, z_latent\n",
    "\n",
    "x = torch.randn(1,10).cuda()\n",
    "y_latent = torch.randn(1,10).cuda()\n",
    "z_latent = torch.randn(1,10).cuda()\n",
    "with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "    y_latent, z_latent = deep_recursion(x, y_latent, z_latent, net=net)\n",
    "    example_class = torch.randint(0, 10, (1,)).cuda()\n",
    "\n",
    "    loss = F.cross_entropy(y_latent, example_class)\n",
    "    loss.backward()\n",
    "    print(net[0].weight.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
