/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(

================================================================================
Training Configuration
================================================================================
  activation: swiglu
  amp: True
  backbone: vit_small
  backbone_checkpoint: /home/derek_austin/dinov3/pretrained_checkpoints/dinov3_vits16_pretrain.pth
  batch_size: 256
  data_root: /home/derek_austin/dinov3/imagenet
  dataset: imagenet
  device: cuda
  epochs: 10
  eval_batch_size: 256
  eval_freq: 1
  experiment_name: trm_imagenet
  freeze_backbone: True
  grad_clip: 1.0
  hidden_dim_multiplier: 1.0
  images_dtype: torch.float32
  latent_y_dim: 256
  latent_z_dim: 256
  learning_rate: 0.0001
  log_interval: 50
  mlp_num_layers: 2
  momentum: 0.9
  n_latent_reasoning_steps: 3
  n_supervision: 4
  num_classes: 1000
  num_workers: 12
  optimizer: adamw
  patch_size: 14
  pin_memory: True
  resolution: 512
  resume: None
  save_dir: ./checkpoints
  save_freq: 2
  save_path: checkpoints/trm_imagenet
  seed: 42
  t_recursion_steps: 2
  use_simple_mlp: False
  use_wandb: False
  wandb_entity: None
  wandb_project: trm-imagenet
  warmup_epochs: 1
  weight_decay: 0.0
================================================================================

Saved configuration to checkpoints/trm_imagenet/config.json

Creating data loaders...
[Warning] Custom resolution 512 used instead of default 448 (for 1024 tokens).
Loading ImageNet dataset from /home/derek_austin/dinov3/imagenet
Train dataset: 1281167 images
Val dataset: 50000 images
Train batches: 5004
Val batches: 196

Building model...
Loading backbone checkpoint from /home/derek_austin/dinov3/pretrained_checkpoints/dinov3_vits16_pretrain.pth
Freezing backbone parameters
Using TRM classifier with 4 supervision steps
Total parameters: 30,344,832
Trainable parameters: 8,743,680

Building optimizer...
Optimizing only TRM parameters (backbone frozen)
latent_y_embedding torch.Size([1, 256])
latent_z_embedding torch.Size([1, 256])
cls_head.weight torch.Size([1000, 256])
q_head.weight torch.Size([1, 256])
net.0.0.weight torch.Size([896, 896])
net.0.1.gate_up_proj.weight torch.Size([2560, 896])
net.0.1.down_proj.weight torch.Size([896, 1280])
net.1.0.weight torch.Size([896, 896])
net.1.1.gate_up_proj.weight torch.Size([2560, 896])
net.1.1.down_proj.weight torch.Size([896, 1280])
Gradient clipping enabled with max norm: 1.0
Using warmup for 1 epochs + cosine decay

================================================================================
Starting Training
================================================================================


Epoch 1/10
--------------------------------------------------------------------------------
Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Epoch [1/10] Batch [50/5004] Loss: 7.6005 Acc: 0.0009 Stop@: 2.59 LR: 0.000001 Speed: 0.59 batch/s
Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Epoch [1/10] Batch [100/5004] Loss: 7.6005 Acc: 0.0009 Stop@: 2.80 LR: 0.000002 Speed: 0.63 batch/s
Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Epoch [1/10] Batch [150/5004] Loss: 7.6003 Acc: 0.0011 Stop@: 3.07 LR: 0.000003 Speed: 0.64 batch/s
Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Epoch [1/10] Batch [200/5004] Loss: 7.6000 Acc: 0.0013 Stop@: 3.30 LR: 0.000004 Speed: 0.65 batch/s
Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan/home/derek_austin/dinov3/.venv/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:760: UserWarning: Metadata Warning, tag 274 had too many entries: 4, expected 1
  warnings.warn(
 False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Epoch [1/10] Batch [250/5004] Loss: 7.5992 Acc: 0.0016 Stop@: 3.44 LR: 0.000005 Speed: 0.65 batch/s
Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(False, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() False
grads have nan after optimizer.step() False
weights have nan False

Loss has nan tensor(True, device='cuda:0')
Grads have nan False
weights have nan False
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Epoch [1/10] Batch [300/5004] Loss: nan Acc: 0.0014 Stop@: 3.13 LR: 0.000006 Speed: 0.66 batch/s
Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Loss has nan tensor(True, device='cuda:0')
Grads have nan True
weights have nan True
grads have nan after loss.backward() True
grads have nan after optimizer.step() True
weights have nan True

Traceback (most recent call last):
  File "/home/derek_austin/dinov3/dinov3/trm/train.py", line 589, in <module>
    main()
  File "/home/derek_austin/dinov3/dinov3/trm/train.py", line 549, in main
    train_metrics = train_epoch(model, train_loader, optimizer, scheduler, epoch, args, scaler, wandb_logger)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/derek_austin/dinov3/dinov3/trm/train.py", line 223, in train_epoch
    images = images.to(device)
             ^^^^^^^^^^^^^^^^^
KeyboardInterrupt
